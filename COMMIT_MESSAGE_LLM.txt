feat: Add local LLM integration via Docker Model Runner

Add optional LLM-powered insight extraction and semantic retrieval using
Llama 3.3 running locally through Docker Model Runner. Falls back to
existing keyword matching if LLM unavailable.

New files:
- llm_client.py: Interface to local Llama via OpenAI-compatible API
- extract_conversation_insights_llm.py: LLM extraction with fallback
- insight_system_llm.py: Semantic ranking extension of existing retrieval
- requirements_llm.txt: Add openai package dependency

Changes:
- No modifications to existing code - all additive
- Keyword matching still default, LLM opt-in
- Performance: 30-60s first query (loads model), 2-5s subsequent

Setup requires:
- Docker Model Runner (Docker Desktop 4.40+)
- docker model pull ai/llama3.3 (~4GB)
- pip install openai

Tests passing on extraction, semantic scoring, and ranking.
